y <- data$medv
x <- data[-y]
View(x)
View(x)
View(x)
View(data)
y <- data[, "medv"]
x <- data[-y]
View(x)
y <- which(data)
y <- data[data["medv"]]
y <- data[["medv"]]
lm.fit = lm(medv~lstat, data = data)
lm.fit = lm(medv~lstat, data = Boston)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit = lm(medv~lstat, data = Boston)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit
summary(Boston)
names(lm.fit)
confint(lm.fit)
predict (lm.fit ,data.frame(lstat =(c(5 ,10 ,15) )),
interval =" confidence ")
predict (lm.fit, data.frame(lstat=(c(5 ,10 ,15))), interval =" confidence ")
predict (lm.fit, data.frame(lstat=(c(5 ,10 ,15))), interval ="confidence")
predict (lm.fit, data.frame(lstat=(c(5 ,10 ,15))), interval ="prediction")
plot(lstat, medv)
abline(lm.fit)
abline (lm.fit ,lwd =3)
plot(lstat ,medv ,col ="red ")
plot(lstat ,medv ,pch =20)
plot(lstat ,medv ,pch ="+")
plot (1:20 ,1:20, pch =1:20)
abline (lm.fit ,lwd =3)
abline (lm.fit ,lwd =3, col ="red ")
par(mfrow =c(2,2))
plot(lm.fit)
plot(predict (lm.fit), residuals (lm.fit))
plot(predict (lm.fit), rstudent (lm.fit))
plot(hatvalues(lm.fit ))
which.max (hatvalues(lm.fit))
?confint
summary(lm.fit)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit
cor(medv, lstat)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit
cor(lstat, medv)
blogdown:::new_post_addin()
install.packages("fpp")
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
library(fpp)
data(elecequip)
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
decompose(elecequip, type = "additive")
plot(decomp)
decomp <- decompose(elecequip, type = "additive")
plot(decomp)
seasonality_adjust <- elecequip - decomp$seasonal
plot(seasonality_adjust)
install.packages("mFilter")
library(mFilter)
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend(1996, 2012, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(elecequip, type = "lambda")$cycle,  xlab = "Time", ylab = "New Orders Index")
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend(1996, 150, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(elecequip, type = "lambda")$cycle,  xlab = "Time", ylab = "New Orders Index")
?legend
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend("topright", 1996, 150, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend(1996, 200, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(elecequip, type = "lambda")$cycle,  xlab = "Time", ylab = "New Orders Index")
data(cafe)
data("cafe")
data("cafe")
hpfilter(cafe, type = "lambda")
par(mfrow= c(2,1))
plot(cafe, xlab = "Time", ylab = "Expenditures Quarters")
lines(hpfilter(cafe, type = "lambda")$trend, col = "red", lwd = 2)
legend(1985, 8000, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(cafe, type = "lambda")$cycle,  xlab = "Time", ylab = "Cycle Component")
blogdown:::new_post_addin()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
BHData <- read.table(url("https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"), sep = "")
str(BHData)
max_data <- apply(BHData, 2, max)
min_data <- apply(BHData, 2, min)
BHDataScaled <- as.data.frame(scale(BHData,center = min_data,
scale = max_data - min_data))
summary(BHDataScaled)
boxplot(BHDataScaled)
names(BHData)<- c("crim","zn","indus","chas","nox","rm",
"age","dis","rad","tax","ptratio","black","lstat","medv")
str(BHData)
summary(BHData)
max_data <- apply(BHData, 2, max)
min_data <- apply(BHData, 2, min)
BHDataScaled <- as.data.frame(scale(BHData,center = min_data,
scale = max_data - min_data))
summary(BHDataScaled)
boxplot(BHDataScaled)
CorBHData<-cor(BHDataScaled)
install.packages("corrplot")
library(corrplot)
corrplot(CorBHData, method = "pie",type="lower")
LModel1<-lm(medv~.,data=BHDataScaled)
summary(LModel1)
Pred1 <- predict(LModel1)
mse1 <- mean((BHDataScaled$medv - Pred1)^2)
mse1
plot(BHDataScaled[,14],Pred1,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
par(mfrow=c(2,2))
plot(LModel1)
install.packages("ramdomForest")
install.packages("randomForest")
library(randomForest)
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel
summary(RFModel)
plot(RFModel)
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp
varImpPlot(RFModel)
Pred2 <- predict(RFModel)
plot(BHDataScaled[,14],Pred2,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
nrows(BHData)
rows(BHData)
nrow(BHData)
ncol(BHData)
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
BHData <- read.table(url("https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"), sep = "")
names(BHData)<- c("crim","zn","indus","chas","nox","rm",
"age","dis","rad","tax","ptratio","black","lstat","medv")
str(BHData)
summary(BHData)
max_data <- apply(BHData, 2, max)
min_data <- apply(BHData, 2, min)
BHDataScaled <- as.data.frame(scale(BHData,center = min_data,
scale = max_data - min_data))
summary(BHDataScaled)
boxplot(BHDataScaled)
CorBHData<-cor(BHDataScaled)
library(corrplot)
corrplot(CorBHData, method = "pie",type="lower")
LModel1<-lm(medv~.,data=BHDataScaled)
summary(LModel1)
Pred1 <- predict(LModel1)
mse1 <- mean((BHDataScaled$medv - Pred1)^2)
mse1
plot(BHDataScaled[,14],Pred1,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
par(mfrow=c(2,2))
plot(LModel1)
library(randomForest)
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel
summary(RFModel)
plot(RFModel)
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp
varImpPlot(RFModel)
Pred2 <- predict(RFModel)
plot(BHDataScaled[,14],Pred2,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
blogdown::serve_site()
remotes::install_github('rstudio/blogdown')
remotes::install_github('rstudio/blogdown')
remove.packages(mime)
remove.packages(mime)
remotes::install_github('rstudio/blogdown')
blogdown::serve
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
dim(BHData)
blogdown::serve_site()
library(MASS)
library(MASS)
LModel2 <- rlm(medv ~., data = "BHDataScaled", psi = psi.hampel, init = "lts")
library(MASS)
LModel2 <- rlm(BHDataScaled$medv ~., data = "BHDataScaled", psi = psi.hampel, init = "lts")
library(MASS)
LModel2 <- rlm(BHDataScaled$medv ~ ., data = "BHDataScaled", psi = psi.hampel, init = "lts")
library(MASS)
LModel2 <- rlm(BHDataScaled$medv ~ ., data = BHDataScaled, psi = psi.hampel, init = "lts")
LModel2
summary(LModel2)
LM1Coef <- coef(LModel1)
LM2Coef <- coef(LModel2)
plot(BHDataScaled$medv, BHDataScaled$lstat)
abline(coef=LM1Coef)
plot(BHDataScaled$medv, BHDataScaled$lstat)
abline(coef=LM2Coef)
boxplot(BHDataScaled$crim)$out
outliers <- boxplot(BHDataScaled$crim, plot=FALSE)$out
outliers <- boxplot(BHDataScaled$crim, plot=FALSE)$out
print(outliers)
BHDataScaled[which(BHDataScaled$crim %in% outliers),]
boxplot(BHDataScaled$crim)
boxplot(BHDataScaled$crim)
remove_outliers <- function(x, na.rm = TRUE, ...) {
qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
H <- 1.5 * IQR(x, na.rm = na.rm)
y <- x
y[x < (qnt[1] - H)] <- NA
y[x > (qnt[2] + H)] <- NA
y
}
remove_outliers(BHDataScaled$crim)
boxplot(BHDataScaled$crim)
boxplot(BHDataScaled$crim)
BHDataScaled$crim[!BHDataScaled$crim %in% boxplot.stats(BHDataScaled$crim)$out]
boxplot(BHDataScaled$crim)
blogdown:::new_post_addin()
library(reticulate)
path <- file.path(Sys.which("python"))
use_python(python = path)
library(reticulate)
path <- file.path(Sys.which("python"))
use_python(python = path)
library(reticulate)
path <- file.path(Sys.which("python"))
use_python(python = path)
library(reticulate)
path <- file.path(Sys.which("python"))
use_python(python = path)
library(reticulate)
path <- file.path(Sys.which("python"))
use_python(python = path)
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
run servr::daemon_stop(2)
blogdown:::new_post_addin()
gewtd()
getwd()
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
path <- "C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/datasets/breast_cancer"
path <- paste(path, wisc_bc_data.csv)
path <- "C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/datasets/breast_cancer"
path <- paste(path, wisc_bc_data.csv)
path <- paste(path, "/wisc_bc_data.csv")
path
path <- paste0(path, "/wisc_bc_data.csv")
path
path <- "C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/datasets/breast_cancer"
path <- paste0(path, "/wisc_bc_data.csv")
path
wbcd <- read.csv(path, stringsAsFactors = FALSE)
str(wbcd)
str(wbcd)
#drop the id column
wbcd <- wbcd[-1]
str(wbcd)
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
> wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
# defining a function
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
library(class)
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
install.packages("gmodels")
library(gmodels)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z$area_mean)
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
dim(wbcd)
path <- "C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/datasets/breast_cancer"
path <- paste0(path, "/wisc_bc_data.csv")
wbcd <- read.csv(path, stringsAsFactors = FALSE)
dim(wbcd)
#drop the id column
wbcd <- wbcd[-1]
str(wbcd)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
# defining a function
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
library(gmodels)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z$area_mean)
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
head(wbcd_test)
head(wbcd_test_pred)
library(caret)
library(caret)
confusionMatrix(wbcd$diagnosis, wbcd_test_pred, positive = "M")
wbcd_test_pred
length(wbcd_test_pred)
library(caret)
confusionMatrix(wbcd_test_labels, wbcd_test_pred, positive = "M")
summary(wbcd_test_labels)
class(wbcd_test_labels)
summary(wbcd_test_pred)
a <- as.vector(wbcd_test_pred)
a
b <- as.vector(wbcd_test_labels)
b
library(caret)
a <- as.vector(wbcd_test_pred)
b <- as.vector(wbcd_test_labels)
confusionMatrix(b, a, positive = "M")
library(caret)
a <- factor(as.vector(wbcd_test_labels), levels = c("B", "M"),
labels = c("Benign", "Malignant"))
b <- factor(as.vector(wbcd_test_pred), levels = c("B", "M"),
labels = c("Benign", "Malignant"))
confusionMatrix(b, a, positive = "M")
library(caret)
a <- factor(as.vector(wbcd_test_labels), levels = c("B", "M"),
labels = c("Benign", "Malignant"))
b <- factor(as.vector(wbcd_test_pred), levels = c("B", "M"),
labels = c("Benign", "Malignant"))
confusionMatrix(b, a, positive = "Malignant")
install.packages("vcd")
library(vcd)
library(vcd)
Kappa(table(a, b))
install.packages("irr")
library(irr)
kappa2(a)
library(irr)
kappa2(a[1:2])
library(irr)
kappa2(a[1:1])
library(irr)
kappa2(c(a,b))
library(caret)
sensitivity(b, a,
positive = "Malignat")
specificity(b, a,
negative = "Benign")
library(caret)
sensitivity(b, a,
positive = "Malignat")
specificity(b, a,
negative = "Benign")
library(caret)
sensitivity(b, a,
positive = "Malignat")
specificity(b, a,
negative = "Benign")
posPredValue(b, b,
positive = "Malignat")
library(caret)
sensitivity(b, a,
positive = "Malignat")
specificity(b, a,
negative = "Benign")
posPredValue(b, a,
positive = "Malignat")
library(caret)
sensitivity(b, a,
positive = "Malignat")
specificity(b, a,
negative = "Benign")
posPredValue(b, a,
positive = "Malignat")
blogdown::serve_site()
path <- "C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/datasets/breast_cancer"
path <- paste0(path, "/wisc_bc_data.csv")
wbcd <- read.csv(path, stringsAsFactors = FALSE)
dim(wbcd)
#drop the id column
wbcd <- wbcd[-1]
str(wbcd)
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
# defining a function
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
head(wbcd_test_pred)
library(gmodels)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
wbcd_z <- as.data.frame(scale(wbcd[-1]))
summary(wbcd_z$area_mean)
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
library(caret)
a <- factor(as.vector(wbcd_test_labels), levels = c("B", "M"),
labels = c("Benign", "Malignant"))
b <- factor(as.vector(wbcd_test_pred), levels = c("B", "M"),
labels = c("Benign", "Malignant"))
confusionMatrix(b, a, positive = "Malignant")
library(vcd)
Kappa(table(a, b))
library(caret)
sensitivity(b, a,
positive = "Malignat")
specificity(b, a,
negative = "Benign")
posPredValue(b, a,
positive = "Malignat")
