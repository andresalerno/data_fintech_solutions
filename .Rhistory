colnames(dados)
dados %>%
mutate(tempo_fundacao = Sys.Date() - DATA_FUNDACAO) %>%
dados
dados %>%
mutate(tempo_fundacao = Sys.Date() - DATA_FUNDACAO) %>%
print(dados)
dados %>%
mutate(tempo_fundacao = Sys.Date() - DATA_FUNDACAO) %>%
View(dados)
lapply(c, d <- a * b)
sapply(c, d <- a * b)
sapply(c, function(a * b))
dados %>%
mutate(tempo_fundacao = Sys.Date() - DATA_FUNDACAO)
colnames(dados)
class(dados)
str(dados)
(Sys.Date() - dados$DATA_FUNDACAO)
dados %>%
mutate(tempo_fundacao = DATA_FUNDACAO + 10)
colnames(dados)
dados %>%
mutate(tempo_fundacao = !is.null(DATA_FUNDACAO + 10)
dados %>%
mutate(tempo_fundacao = !is.null(DATA_FUNDACAO + 10))
colnames(dados)
dados %>%
mutate(tempo_fundacao = is.null(DATA_FUNDACAO))
colnames(dados)
dados %>%
dados <- mutate(tempo_fundacao = is.null(DATA_FUNDACAO))
dados %>%
dados <- mutate(tempo_fundacao = is.null(dados$DATA_FUNDACAO))
dados %>%
mutate(tempo_fundacao = DATA_FUNDACAO)
colnames(dados)
dados %>%
mutate(tempo_fundacao = DATA_FUNDACAO)
dados %>%
!is.na(mutate(tempo_fundacao = DATA_FUNDACAO))
any(is.na(dados$DATA_FUNDACAO))
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
index_dt_fundacao_nulls
length(index_dt_fundacao_nulls)
dados$DATA_FUNDACAO[106556]
dados$DATA_FUNDACAO[171]
dados$DATA_FUNDACAO[172]
any(is.na(dados$DATA_FUNDACAO)) # checking NULLs
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
length(index_dt_fundacao_nulls) # number of NULLs
dados %>%
if (dados$DATA_FUNDACAO == !NA){
mutate(tempo_fundacao = (Sys.Date() - DATA_FUNDACAO) / 365)
} else{
mutate(tempo_fundacao = "")
}
any(is.na(dados$DATA_FUNDACAO)) # checking NULLs
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
length(index_dt_fundacao_nulls) # number of NULLs
dados %>%
if (DATA_FUNDACAO == !NA){
mutate(tempo_fundacao = (Sys.Date() - DATA_FUNDACAO) / 365)
} else{
mutate(tempo_fundacao = "")
}
any(is.na(dados$DATA_FUNDACAO)) # checking NULLs
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
length(index_dt_fundacao_nulls) # number of NULLs
if (dados$DATA_FUNDACAO == !NA){
mutate(tempo_fundacao = (Sys.Date() - DATA_FUNDACAO) / 365)
} else{
mutate(tempo_fundacao = "")
}
any(is.na(dados$DATA_FUNDACAO)) # checking NULLs
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
length(index_dt_fundacao_nulls) # number of NULLs
if (dados$DATA_FUNDACAO !NA){
df <- data.frame(col1 = c("abc","abcd","a","abcdefg"),col2 = c("adf qqwe","d","e","f"))
for(i in names(df)){
df[[paste(i, 'length', sep="_")]] <- str_length(df[[i]])
}
View(df)
library(stringr)
out <- lapply( df , str_length )
df <- cbind( df , out )
View(df)
any(is.na(dados$DATA_FUNDACAO)) # checking NULLs
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
length(index_dt_fundacao_nulls) # number of NULLs
if (dados$DATA_FUNDACAO !NA){
any(is.na(dados$DATA_FUNDACAO)) # checking NULLs
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
length(index_dt_fundacao_nulls) # number of NULLs
if (dados$DATA_FUNDACAO == !NA){
mutate_all(tempo_fundacao = (Sys.Date() - DATA_FUNDACAO) / 365)
} else{
mutate_all(tempo_fundacao = "")
}
any(is.na(dados$DATA_FUNDACAO)) # checking NULLs
index_dt_fundacao_nulls <- which(is.na(dados$DATA_FUNDACAO)) # Locate NULLs
length(index_dt_fundacao_nulls) # number of NULLs
if (dados$DATA_FUNDACAO == !NA){
mutate_all(tempo_fundacao = (Sys.Date() - DATA_FUNDACAO) / 365)
} else{
mutate_all(tempo_fundacao = "")
}
a <- rnorm(100, 5, 2)
a
b <- rnorm(200, 20, 4)
df <- data.frame(a, b)
df
df %>%
mutate(c = a * b)
df
df
df %>%
mutate_all(c = a * b)
df %>%
mutate_all(c = a * b)
df
df %>%
mutate_all(.funs = a * b)
df
df %>%
mutate(c = a * b)
df
df %>%
df <- mutate(c = a * b)
df %>%
df <- mutate(c = a * b)
df %>%
mutate(c = a * b)
df %>%
mutate(c = a * b) %>%
df
dados %>%
as.tibble() %>%
mutate(
tempo_fundacao = (Sys.Date() - dados$DATA_FUNDACAO) / 365, na.rm = TRUE))
dados %>%
as.tibble() %>%
mutate(
tempo_fundacao = ((Sys.Date() - dados$DATA_FUNDACAO) / 365), na.rm = TRUE))
install.packages("blogdown")
options(blogdown.ext = ".Rmd", blogdown.author = "Andr√© Salerno, MSc")
blogdown:::new_post_addin()
library(reticulate)
path <- file.path(Sys.which("python"))
use_python(python = path)
setwd("C:/data_fintech_solutions")
install.packages("pROC")
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
library(pROC)
# Load the pROC package
library(pROC)
# Create a ROC curve
ROC <- roc(donors$donated, donors$donation_prob)
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
knitr::include_graphics("/data_fintech_solutions/temp.png")
library(class)
signs <- read.csv(file = "/data_fintech_solutions/knn_traffic_signs.csv", header = TRUE, sep = ",")
dim(signs)
names(signs)
# get the number of observations
n_obs <- nrow(signs)
# Shuffle row indices: permuted_rows
permuted_rows <- sample(n_obs)
# Randomly order data: signs_shuffled
signs_shuffled <- signs[permuted_rows, ]
# Identify row to split on: split
split <- round(n_obs * 0.7)
# Create train
train <- signs_shuffled[1:split, 3:51]
# Create test
test <- signs_shuffled[(split + 1):n_obs, 3:51]
# Create a vector of labels
sign_types <- signs_shuffled$sign_type[1:split]
next_sign <- signs_shuffled[206, 3:51]
# Classify the next sign observed
signs_pred <- knn(train = train[-1], test = test[-1], cl = sign_types)
knitr::include_graphics("/data_fintech_solutions/temp_1.png")
# Examine the structure of the signs dataset
str(signs)
# Count the number of signs of each type
table(signs$sign_type)
# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
knitr::include_graphics("/data_fintech_solutions/temp_2.png")
knitr::include_graphics("/data_fintech_solutions/temp_3.png")
knitr::include_graphics("/data_fintech_solutions/temp_4.png")
# Create a confusion matrix of the predicted versus actual values
signs_actual <- test$sign_type
table(signs_actual, signs_pred)
# Compute the accuracy
mean(signs_actual == signs_pred)
# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = train[-1], test = test[-1], cl = sign_types)
mean(signs_actual == k_1)
# Modify the above to set k = 7
k_7 <- knn(train = train[-1], test = test[-1], cl = sign_types, k = 7)
mean(signs_actual == k_7)
# Set k = 15 and compare to the above
k_15 <- knn(train = train[-1], test = test[-1], cl = sign_types, k = 15)
mean(signs_actual == k_15)
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train = train[-1], test = test[-1], cl= sign_types, k = 7, prob = TRUE)
# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")
# Examine the first several predictions
head(sign_pred)
# Examine the proportion of votes for the winning class
head(sign_prob)
where9am <- read.csv(file = "/data_fintech_solutions/locations.csv", header = TRUE, sep = ",")
dim(where9am)
head(where9am)
str(where9am)
# Compute P(A)
p_A <- nrow(subset(where9am, location == "office")) / nrow(where9am)
# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday")) / nrow(where9am)
# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office" & daytype == "weekday")) / nrow(where9am)
# Compute P(A | B) and print its value
p_A_given_B <- p_AB / p_B
p_A_given_B
thursday9am <- data.frame(weekday = "thursday", stringsAsFactors = FALSE)
saturday9am <- data.frame(weekday = "saturday", stringsAsFactors = FALSE)
# Load the naivebayes package
library(naivebayes)
# Build the location prediction model
locmodel <- naive_bayes(location ~ weekday, data = where9am, laplace = 1)
# Predict Thursday's 9am location
predict(locmodel, thursday9am)
# Predict Saturdays's 9am location
predict(locmodel, saturday9am)
# Examine the location prediction model
print(locmodel)
# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am, type = "prob")
# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am, type = "prob")
weekday_afternoon <- data.frame(daytype = "weekday",
hourtype = "afternoon",
location = "office", stringsAsFactors = FALSE)
weekday_evening <- data.frame(daytype = "weekday",
hourtype = "evening",
location = "home", stringsAsFactors = FALSE)
# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, where9am, laplace = 0)
# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)
# Predict Brett's location on a weekday evening
predict(locmodel, weekday_evening)
weekend_afternoon <- data.frame(daytype = "weekend",
hourtype = "afternoon",
location = "home", stringsAsFactors = FALSE)
weekend_evening <- data.frame(daytype = "weekend",
hourtype = "evening",
location = "home", stringsAsFactors = FALSE)
# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, weekend_afternoon, type = "prob")
# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, where9am, laplace = 1)
# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type = "prob")
donors <- read.csv(file = "/data_fintech_solutions/donors.csv", header = TRUE, sep = ",")
# Examine the dataset to identify potential independent variables
str(donors)
# Explore the dependent variable
table(donors$donated)
# Build the donation model
donation_model <- glm(donated ~ bad_address + interest_religion + interest_veterans,
data = donors, family = "binomial")
# Summarize the model results
summary(donation_model)
# Estimate the donation probability
donors$donation_prob <- predict(donation_model, type = "response")
# Find the donation probability of the average prospect
mean(donors$donated)
# Predict a donation if probability of donation is greater than average (0.0504)
donors$donation_pred <- ifelse(donors$donation_prob > 0.0504, 1, 0)
# Calculate the model's accuracy
mean(donors$donation_pred == donors$donated)
library(dplyr)
prop <- table(donors$donated)
round(prop.table(prop) * 100, 3)
# Load the pROC package
library(pROC)
# Create a ROC curve
ROC <- roc(donors$donated, donors$donation_prob)
# Plot the ROC curve
plot(ROC, col = "blue")
# Calculate the area under the curve (AUC)
auc(ROC)
knitr::include_graphics("/data_fintech_solutions/lr_auc_compare.png")
# Convert the wealth rating to a factor
donors$wealth_levels <- factor(donors$wealth_rating, levels = c(0, 1, 2, 3), labels = c("Unknown", "Low", "Medium", "High"))
# Use relevel() to change reference category
donors$wealth_levels <- relevel(donors$wealth_levels, ref = "Medium")
# See how our factor coding impacts the model
summary(glm(donated ~ wealth_levels, donors, family = "binomial"))
sumary(donors$age)
summary(donors$age)
# Find the average age among non-missing values
summary(donors$age)
# Impute missing age values with the mean age
donors$imputed_age <- ifelse(is.na(donors$age), round(mean(donors$age, na.rm = TRUE), 2), donors$age)
# Create missing value indicator for age
donors$missing_age <- ifelse(is.na(donors$age), 1, 0)
# Build a recency, frequency, and money (RFM) model
rfm_model <- glm(donated ~ money + recency * frequency, family = "binomial", donors)
# Summarize the RFM model to see how the parameters were coded
summary(rfm_model)
# Compute predicted probabilities for the RFM model
rfm_prob <- predict(rfm_model, type = "response")
# Plot the ROC curve and find AUC for the new model
library(pROC)
ROC <- roc(donors$donated, rfm_prob)
plot(ROC, col = "red")
auc(ROC)
# Specify a null model with no predictors
null_model <- glm(donated ~ 1, data = donors, family = "binomial")
# Specify the full model using all of the potential predictors
full_model <- glm(donated ~ ., donors, family= "binomial")
# Use a forward stepwise algorithm to build a parsimonious model
step_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")
# Estimate the stepwise donation probability
step_prob <- predict(step_model, type = "response")
# Plot the ROC of the stepwise model
library(pROC)
ROC <- roc(donors$donated, step_prob)
plot(ROC, col = "red")
auc(ROC)
blogdown::serve_site()
blogdown::serve_site()
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
knitr::include_graphics("/data_fintech_solutions/temp.png")
library(class)
signs <- read.csv(file = "/data_fintech_solutions/knn_traffic_signs.csv", header = TRUE, sep = ",")
dim(signs)
names(signs)
# get the number of observations
n_obs <- nrow(signs)
# Shuffle row indices: permuted_rows
permuted_rows <- sample(n_obs)
# Randomly order data: signs_shuffled
signs_shuffled <- signs[permuted_rows, ]
# Identify row to split on: split
split <- round(n_obs * 0.7)
# Create train
train <- signs_shuffled[1:split, 3:51]
# Create test
test <- signs_shuffled[(split + 1):n_obs, 3:51]
# Create a vector of labels
sign_types <- signs_shuffled$sign_type[1:split]
next_sign <- signs_shuffled[206, 3:51]
# Classify the next sign observed
signs_pred <- knn(train = train[-1], test = test[-1], cl = sign_types)
knitr::include_graphics("/data_fintech_solutions/temp_1.png")
# Examine the structure of the signs dataset
str(signs)
# Count the number of signs of each type
table(signs$sign_type)
# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
knitr::include_graphics("/data_fintech_solutions/temp_2.png")
knitr::include_graphics("/data_fintech_solutions/temp_3.png")
knitr::include_graphics("/data_fintech_solutions/temp_4.png")
# Create a confusion matrix of the predicted versus actual values
signs_actual <- test$sign_type
table(signs_actual, signs_pred)
# Compute the accuracy
mean(signs_actual == signs_pred)
# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = train[-1], test = test[-1], cl = sign_types)
mean(signs_actual == k_1)
# Modify the above to set k = 7
k_7 <- knn(train = train[-1], test = test[-1], cl = sign_types, k = 7)
mean(signs_actual == k_7)
# Set k = 15 and compare to the above
k_15 <- knn(train = train[-1], test = test[-1], cl = sign_types, k = 15)
mean(signs_actual == k_15)
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train = train[-1], test = test[-1], cl= sign_types, k = 7, prob = TRUE)
# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")
# Examine the first several predictions
head(sign_pred)
# Examine the proportion of votes for the winning class
head(sign_prob)
where9am <- read.csv(file = "/data_fintech_solutions/locations.csv", header = TRUE, sep = ",")
dim(where9am)
head(where9am)
str(where9am)
# Compute P(A)
p_A <- nrow(subset(where9am, location == "office")) / nrow(where9am)
# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday")) / nrow(where9am)
# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office" & daytype == "weekday")) / nrow(where9am)
# Compute P(A | B) and print its value
p_A_given_B <- p_AB / p_B
p_A_given_B
thursday9am <- data.frame(weekday = "thursday", stringsAsFactors = FALSE)
saturday9am <- data.frame(weekday = "saturday", stringsAsFactors = FALSE)
# Load the naivebayes package
library(naivebayes)
# Build the location prediction model
locmodel <- naive_bayes(location ~ weekday, data = where9am, laplace = 1)
# Predict Thursday's 9am location
predict(locmodel, thursday9am)
# Predict Saturdays's 9am location
predict(locmodel, saturday9am)
# Examine the location prediction model
print(locmodel)
# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am, type = "prob")
# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am, type = "prob")
weekday_afternoon <- data.frame(daytype = "weekday",
hourtype = "afternoon",
location = "office", stringsAsFactors = FALSE)
weekday_evening <- data.frame(daytype = "weekday",
hourtype = "evening",
location = "home", stringsAsFactors = FALSE)
# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, where9am, laplace = 0)
# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)
# Predict Brett's location on a weekday evening
predict(locmodel, weekday_evening)
weekend_afternoon <- data.frame(daytype = "weekend",
hourtype = "afternoon",
location = "home", stringsAsFactors = FALSE)
weekend_evening <- data.frame(daytype = "weekend",
hourtype = "evening",
location = "home", stringsAsFactors = FALSE)
# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, weekend_afternoon, type = "prob")
# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, where9am, laplace = 1)
# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type = "prob")
donors <- read.csv(file = "/data_fintech_solutions/donors.csv", header = TRUE, sep = ",")
# Examine the dataset to identify potential independent variables
str(donors)
# Explore the dependent variable
table(donors$donated)
# Build the donation model
donation_model <- glm(donated ~ bad_address + interest_religion + interest_veterans,
data = donors, family = "binomial")
# Summarize the model results
summary(donation_model)
# Estimate the donation probability
donors$donation_prob <- predict(donation_model, type = "response")
# Find the donation probability of the average prospect
mean(donors$donated)
# Predict a donation if probability of donation is greater than average (0.0504)
donors$donation_pred <- ifelse(donors$donation_prob > 0.0504, 1, 0)
# Calculate the model's accuracy
mean(donors$donation_pred == donors$donated)
library(dplyr)
prop <- table(donors$donated)
round(prop.table(prop) * 100, 3)
# Load the pROC package
library(pROC)
# Create a ROC curve
ROC <- roc(donors$donated, donors$donation_prob)
# Plot the ROC curve
plot(ROC, col = "blue")
# Calculate the area under the curve (AUC)
auc(ROC)
knitr::include_graphics("/data_fintech_solutions/lr_auc_compare.png")
# Convert the wealth rating to a factor
donors$wealth_levels <- factor(donors$wealth_rating, levels = c(0, 1, 2, 3), labels = c("Unknown", "Low", "Medium", "High"))
# Use relevel() to change reference category
donors$wealth_levels <- relevel(donors$wealth_levels, ref = "Medium")
# See how our factor coding impacts the model
summary(glm(donated ~ wealth_levels, donors, family = "binomial"))
summary(donors$age)
# Find the average age among non-missing values
summary(donors$age)
# Impute missing age values with the mean age
donors$imputed_age <- ifelse(is.na(donors$age), round(mean(donors$age, na.rm = TRUE), 2), donors$age)
# Create missing value indicator for age
donors$missing_age <- ifelse(is.na(donors$age), 1, 0)
# Build a recency, frequency, and money (RFM) model
rfm_model <- glm(donated ~ money + recency * frequency, family = "binomial", donors)
# Summarize the RFM model to see how the parameters were coded
summary(rfm_model)
# Compute predicted probabilities for the RFM model
rfm_prob <- predict(rfm_model, type = "response")
# Plot the ROC curve and find AUC for the new model
library(pROC)
ROC <- roc(donors$donated, rfm_prob)
plot(ROC, col = "red")
auc(ROC)
# Specify a null model with no predictors
null_model <- glm(donated ~ 1, data = donors, family = "binomial")
# Specify the full model using all of the potential predictors
full_model <- glm(donated ~ ., donors, family= "binomial")
# Use a forward stepwise algorithm to build a parsimonious model
step_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")
# Estimate the stepwise donation probability
step_prob <- predict(step_model, type = "response")
# Plot the ROC of the stepwise model
library(pROC)
ROC <- roc(donors$donated, step_prob)
plot(ROC, col = "red")
auc(ROC)
blogdown::serve_site()
