library(blogdown)
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::update_meta_addin()
blogdown::serve_site()
run servr::daemon_stop(2)
run servr::daemon_stop
run servr::daemon_stop(2)
blogdown::serve_site()
setwd("C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown")
getwd()
system("rm -r C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/public/*")
blogdown::hugo_build(local=F)
system("C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/deploy.sh")
blogdown::hugo_version()
blogdown::serve_site()
blogdown::serve_site()
system("rm -r C:/Users/andre/OneDrive/Área de Trabalho/salerno/blogdown/public/*")
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
bookdown:::serve_book()
library(blogdown)
blogdown::serve_site()
getwd()
knitr::opts_chunk$set(collapse = TRUE)
data <- read.csv(file = paste0(path, "\Advertising.csv"), header = TRUE, sep = ",")
path <- file.path("C:\Users\andre\OneDrive\Documentos\Private\Salerno\Pessoal\Livros\An Introduction to Statistical Learning  with Applications in R\")
path <- file.path("C:\Users\andre\OneDrive\Documentos\Private\Salerno\Pessoal\Livros\An Introduction to Statistical Learning  with Applications in R\")
path <- file.path("C:\\Users\\andre\\OneDrive\\Documentos\\Private\\Salerno\\Pessoal\\Livros\\An Introduction to Statistical Learning  with Applications in R\\")
data <- read.csv(file = paste0(path, "Advertising.csv"), header = TRUE, sep = ",")
View(data)
data <- read.csv(file = paste0(path, "Advertising.csv"), header = TRUE, sep = ";")
data <- read.csv(file = paste0(path, "Advertising.csv"), header = TRUE, sep = ",")
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
path <- file.path("C:\\Users\\andre\\OneDrive\\Documentos\\Private\\Salerno\\Pessoal\\Livros\\An Introduction to Statistical Learning  with Applications in R\\")
data <- read.csv(file = paste0(path, "Advertising.csv"), header = TRUE, sep = ",")
install.packages("MASS")
library(MASS)
```
data <- MASS::Boston
fix(Boston)
names(Boston)
y <- data$medv
x <- data[-y]
View(x)
View(x)
View(x)
View(data)
y <- data[, "medv"]
x <- data[-y]
View(x)
y <- which(data)
y <- data[data["medv"]]
y <- data[["medv"]]
lm.fit = lm(medv~lstat, data = data)
lm.fit = lm(medv~lstat, data = Boston)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit = lm(medv~lstat, data = Boston)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit
summary(Boston)
names(lm.fit)
confint(lm.fit)
predict (lm.fit ,data.frame(lstat =(c(5 ,10 ,15) )),
interval =" confidence ")
predict (lm.fit, data.frame(lstat=(c(5 ,10 ,15))), interval =" confidence ")
predict (lm.fit, data.frame(lstat=(c(5 ,10 ,15))), interval ="confidence")
predict (lm.fit, data.frame(lstat=(c(5 ,10 ,15))), interval ="prediction")
plot(lstat, medv)
abline(lm.fit)
abline (lm.fit ,lwd =3)
plot(lstat ,medv ,col ="red ")
plot(lstat ,medv ,pch =20)
plot(lstat ,medv ,pch ="+")
plot (1:20 ,1:20, pch =1:20)
abline (lm.fit ,lwd =3)
abline (lm.fit ,lwd =3, col ="red ")
par(mfrow =c(2,2))
plot(lm.fit)
plot(predict (lm.fit), residuals (lm.fit))
plot(predict (lm.fit), rstudent (lm.fit))
plot(hatvalues(lm.fit ))
which.max (hatvalues(lm.fit))
?confint
summary(lm.fit)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit
cor(medv, lstat)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit
cor(lstat, medv)
blogdown:::new_post_addin()
install.packages("fpp")
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
library(fpp)
data(elecequip)
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
decompose(elecequip, type = "additive")
plot(decomp)
decomp <- decompose(elecequip, type = "additive")
plot(decomp)
seasonality_adjust <- elecequip - decomp$seasonal
plot(seasonality_adjust)
install.packages("mFilter")
library(mFilter)
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend(1996, 2012, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(elecequip, type = "lambda")$cycle,  xlab = "Time", ylab = "New Orders Index")
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend(1996, 150, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(elecequip, type = "lambda")$cycle,  xlab = "Time", ylab = "New Orders Index")
?legend
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend("topright", 1996, 150, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
library(mFilter)
hpfilter(elecequip, type = "lambda")
par(mfrow= c(2,1))
plot(elecequip, xlab = "Time", ylab = "New Orders Index")
lines(hpfilter(elecequip, type = "lambda")$trend, col = "red", lwd = 2)
legend(1996, 200, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(elecequip, type = "lambda")$cycle,  xlab = "Time", ylab = "New Orders Index")
data(cafe)
data("cafe")
data("cafe")
hpfilter(cafe, type = "lambda")
par(mfrow= c(2,1))
plot(cafe, xlab = "Time", ylab = "Expenditures Quarters")
lines(hpfilter(cafe, type = "lambda")$trend, col = "red", lwd = 2)
legend(1985, 8000, c("Original Serie", "Trend - HP Filter"), col = c("black", "red"), lwd = c(1,2), bty = "n")
plot(hpfilter(cafe, type = "lambda")$cycle,  xlab = "Time", ylab = "Cycle Component")
blogdown:::new_post_addin()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
BHData <- read.table(url("https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"), sep = "")
str(BHData)
max_data <- apply(BHData, 2, max)
min_data <- apply(BHData, 2, min)
BHDataScaled <- as.data.frame(scale(BHData,center = min_data,
scale = max_data - min_data))
summary(BHDataScaled)
boxplot(BHDataScaled)
names(BHData)<- c("crim","zn","indus","chas","nox","rm",
"age","dis","rad","tax","ptratio","black","lstat","medv")
str(BHData)
summary(BHData)
max_data <- apply(BHData, 2, max)
min_data <- apply(BHData, 2, min)
BHDataScaled <- as.data.frame(scale(BHData,center = min_data,
scale = max_data - min_data))
summary(BHDataScaled)
boxplot(BHDataScaled)
CorBHData<-cor(BHDataScaled)
install.packages("corrplot")
library(corrplot)
corrplot(CorBHData, method = "pie",type="lower")
LModel1<-lm(medv~.,data=BHDataScaled)
summary(LModel1)
Pred1 <- predict(LModel1)
mse1 <- mean((BHDataScaled$medv - Pred1)^2)
mse1
plot(BHDataScaled[,14],Pred1,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
par(mfrow=c(2,2))
plot(LModel1)
install.packages("ramdomForest")
install.packages("randomForest")
library(randomForest)
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel
summary(RFModel)
plot(RFModel)
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp
varImpPlot(RFModel)
Pred2 <- predict(RFModel)
plot(BHDataScaled[,14],Pred2,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
nrows(BHData)
rows(BHData)
nrow(BHData)
ncol(BHData)
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
BHData <- read.table(url("https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"), sep = "")
names(BHData)<- c("crim","zn","indus","chas","nox","rm",
"age","dis","rad","tax","ptratio","black","lstat","medv")
str(BHData)
summary(BHData)
max_data <- apply(BHData, 2, max)
min_data <- apply(BHData, 2, min)
BHDataScaled <- as.data.frame(scale(BHData,center = min_data,
scale = max_data - min_data))
summary(BHDataScaled)
boxplot(BHDataScaled)
CorBHData<-cor(BHDataScaled)
library(corrplot)
corrplot(CorBHData, method = "pie",type="lower")
LModel1<-lm(medv~.,data=BHDataScaled)
summary(LModel1)
Pred1 <- predict(LModel1)
mse1 <- mean((BHDataScaled$medv - Pred1)^2)
mse1
plot(BHDataScaled[,14],Pred1,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
par(mfrow=c(2,2))
plot(LModel1)
library(randomForest)
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel
summary(RFModel)
plot(RFModel)
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp
varImpPlot(RFModel)
Pred2 <- predict(RFModel)
plot(BHDataScaled[,14],Pred2,
xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
blogdown::serve_site()
remotes::install_github('rstudio/blogdown')
remotes::install_github('rstudio/blogdown')
remove.packages(mime)
remove.packages(mime)
remotes::install_github('rstudio/blogdown')
blogdown::serve
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
