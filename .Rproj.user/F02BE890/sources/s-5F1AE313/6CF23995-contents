---
title: "Random Forest"
author: "Salerno"
date: "2019-12-23"
categories: ["R", "programming"]
tags: ["R Markdown", "random forest", "regression"]
draft: FALSE
banner: img/banners/banner-5.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
```

# Random Forest

In this post we will explore some ideas around the Random Forest model

## Objective

We are working on in the dataset called *Boston Housing* and the main idea here is regression task and we are concerned with modeling the price of houses in thousands of dollars in the Surburb of Boston.

So, we are dirting our hands in a regression predictive modeling problem.

The main goal here is to fit a regression model that best explains the variation in `medv` variable.

## Data

In terms of dataset, we are using a file from *UCI* and their content is related of Housing Values in Suburbs of Boston.

```{r}
# to get the data
BHData <- read.table(url("https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"), sep = "")

```

For our study we are working on 506 rows (events) and 14 columns. One of them called `medv` is our target value - `y` or response variable.

```{r}
# knowing the dimension of the data
dim(BHData)
```


## Set names of the dataset

```{r}
# changing the variable's names
names(BHData)<- c("crim","zn","indus","chas","nox","rm",   
       "age","dis","rad","tax","ptratio","black","lstat","medv")
```

## EDA (Exploratory Data Analysis)

Usually as a first task, we use some *Exploratory Data Analysis* to understand how the data is distributed and extract preliminary knowledge.

```{r}
# structure
str(BHData)
```

As you can see using the `summary()` function below, there are variables with different ranges. It is could badly impact the response variable if we have had a less numeric range between each of the predictors variables.

As we have to improve the predictive accuracy of our model we have not allowed that this large difference in the range of variables impact the accuracy of the predicting task upon the `medv` variable.

You will see the adequate treatment in the *Pre-processing* topic.

```{r}
summary(BHData)
```

## Pre-processing

### Remove outliers

```{r}
boxplot(BHData$crim,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 25),
        main = "Dispersion in CRIM Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$zn,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 100),
        main = "Dispersion in ZN Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$indus,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 50),
        main = "Dispersion in INDUS Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$chas,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 10),
        main = "Dispersion in CHAS Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$nox,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 1),
        main = "Dispersion in NOX Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$rm,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 10),
        main = "Dispersion in RM Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$age,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 100),
        main = "Dispersion in AGE Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$dis,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 15),
        main = "Dispersion in DIS Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$rad,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 50),
        main = "Dispersion in RAD Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$tax,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 50),
        main = "Dispersion in TAX Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$ptratio,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 50),
        main = "Dispersion in PTRATIO Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$black,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 50),
        main = "Dispersion in BLACK Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$zn,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 50),
        main = "Dispersion in ZN Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
boxplot(BHData$medv,
        boxfill = "light gray",
        outpch = 21:25,
        outlty = 2,
        bg = "pink",
        lwd = 2,
        medcol = "dark green",
        medcex = 2,
        medpch = 20,
        ylim = c(0, 50),
        main = "Dispersion in MEDV Variable w/ Outlier Detection",
        range = 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        cex.axis = 1.5)
```

```{r}
#cut-off values are given by the formal definition of an outlier: 
#Q3 + 1.5*IQR
upper_cut_off1 <- (quantile(BHData$crim, 0.75)) + (IQR(BHData$crim))*1.5
upper_cut_off1


upper_cut_off2 <- (quantile(BHData$zn, 0.75)) + (IQR(BHData$zn))*1.5
upper_cut_off2


upper_cut_off3 <- (quantile(BHData$indus, 0.75)) + (IQR(BHData$indus))*1.5
upper_cut_off3


upper_cut_off4 <- (quantile(BHData$chas, 0.75)) + (IQR(BHData$chas))*1.5
upper_cut_off4
```


### Feature Scaling

It is an important step called `featured scaling` to get all the data scaled in the range [0,1]. This method has chosen can be called as well as `normalization`. 

```{r}
# calculating the maximun in each column
max_data <- apply(BHData, 2, max)
```

```{r}
# calculating the minimun in each column
min_data <- apply(BHData, 2, min)
```

```{r}
# applying the normalization
BHDataScaled <- as.data.frame(scale(BHData,center = min_data, 
  scale = max_data - min_data))
```

To confirm normalization process:

```{r}
summary(BHDataScaled)
```

```{r}
boxplot(BHDataScaled)
```

According with the graph above there some som variables with outliers. But the `crim` predictor variable has the largest number os outliers.

```{r}
CorBHData<-cor(BHDataScaled)
```

```{r}
library(corrplot)

corrplot(CorBHData, method = "pie",type="lower")
```

## Multiple Linear Model Fitting

```{r}
LModel1<-lm(medv~.,data=BHDataScaled)
```

```{r}
summary(LModel1)
```

```{r}
Pred1 <- predict(LModel1)
```

```{r}
mse1 <- mean((BHDataScaled$medv - Pred1)^2)
```

```{r}
mse1
```

```{r}
plot(BHDataScaled[,14],Pred1,
     xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
```

```{r}
par(mfrow=c(2,2))
plot(LModel1)
```

## Random Forest Regression Model

```{r}
library(randomForest)
```

```{r}
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel
```

```{r}
summary(RFModel)
```

```{r}
plot(RFModel)
```

```{r}
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp
```

```{r}
varImpPlot(RFModel)
```

```{r}
Pred2 <- predict(RFModel)
```

```{r}
plot(BHDataScaled[,14],Pred2,
     xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
```



## Robust Linear Regression Model

```{r}
library(MASS)

LModel2 <- rlm(BHDataScaled$medv ~ ., data = BHDataScaled, psi = psi.hampel, init = "lts")
```

```{r}
LModel2
```

```{r}
summary(LModel2)
```

```{r}
LM1Coef <- coef(LModel1)

LM2Coef <- coef(LModel2)
```

```{r}
plot(BHDataScaled$medv, BHDataScaled$lstat)
abline(coef=LM1Coef)
plot(BHDataScaled$medv, BHDataScaled$lstat)
abline(coef=LM2Coef)
```

```{r}
boxplot(BHDataScaled$crim)$out
```

```{r}
outliers <- boxplot(BHDataScaled$crim, plot=FALSE)$out
print(outliers)
```


```{r}
BHDataScaled[which(BHDataScaled$crim %in% outliers),]
```

```{r}
boxplot(BHDataScaled$crim)
```

```{r}
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
```

```{r}
remove_outliers(BHDataScaled$crim)
```

```{r}
BHDataScaled$crim[!BHDataScaled$crim %in% boxplot.stats(BHDataScaled$crim)$out]
```

## Car Dataset

```{r}
# Data Source: https://archive.ics.uci.edu/ml/machine-learning-databases/car/
 
library(randomForest)
```

```{r}
# Load the dataset and explore
data1 <- read.table(url("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data"), sep = ",")

names(data1) <- c("BuyingPrice", "Maintenance", "NumDoors", "NUmPersons", "BootSpace", "Safety", "Condition")

head(data1)
 
str(data1)
 
summary(data1)

```

```{r}
# Split into Train and Validation sets
# Training Set : Validation Set = 70 : 30 (random)
set.seed(100)
train <- sample(nrow(data1), 0.7*nrow(data1), replace = FALSE)
TrainSet <- data1[train,]
ValidSet <- data1[-train,]
summary(TrainSet)
summary(ValidSet)
```

```{r}
summary(TrainSet)
summary(ValidSet)
```

```{r}
# Create a Random Forest model with default parameters
model1 <- randomForest(Condition ~ ., data = TrainSet, importance = TRUE)
model1
```

```{r}
# Fine tuning parameters of Random Forest model
model2 <- randomForest(Condition ~ ., data = TrainSet, ntree = 500, mtry = 6, importance = TRUE)
model2
```

```{r}
# Predicting on train set
predTrain <- predict(model2, TrainSet, type = "class")
# Checking classification accuracy
table(predTrain, TrainSet$Condition)  
```

```{r}
# Predicting on Validation set
predValid <- predict(model2, ValidSet, type = "class")
# Checking classification accuracy
mean(predValid == ValidSet$Condition)                    
table(predValid,ValidSet$Condition)
```

```{r}
# To check important variables
importance(model2)        
varImpPlot(model2)   
```

```{r}
# Using For loop to identify the right mtry for model
a=c()
i=5
for (i in 3:8) {
  model3 <- randomForest(Condition ~ ., data = TrainSet, ntree = 500, mtry = i, importance = TRUE)
  predValid <- predict(model3, ValidSet, type = "class")
  a[i-2] = mean(predValid == ValidSet$Condition)
}
 
a
 
plot(3:8,a)
```

```{r}
library(rpart)
library(caret)
library(e1071)

# We will compare model 1 of Random Forest with Decision Tree model
 
model_dt = train(Condition ~ ., data = TrainSet, method = "rpart")
model_dt_1 = predict(model_dt, data = TrainSet)
table(model_dt_1, TrainSet$Condition)
 
mean(model_dt_1 == TrainSet$Condition)
```

```{r}
# Running on Validation Set
model_dt_vs = predict(model_dt, newdata = ValidSet)
table(model_dt_vs, ValidSet$Condition)
 
mean(model_dt_vs == ValidSet$Condition)
```

