---
title: "Random Forest"
author: "Salerno"
date: "2019-12-23"
categories: ["R", "programming"]
tags: ["R Markdown", "random forest", "regression"]
draft: FALSE
banner: img/banners/banner-5.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warnings = FALSE, echo = TRUE)
```

# Random Forest

In this post we will explore some ideas around the Random Forest model

## Objective

We are working on in the dataset called *Boston Housing* and the main idea here is regression task and we are concerned with modeling the price of houses in thousands of dollars in the Surburb of Boston.

So, we are dirting our hands in a regression predictive modeling problem.

The main goal here is to fit a regression model that best explains the variation in `medv` variable.

## Data

In terms of dataset, we are using a file from *UCI* and their content is related of Housing Values in Suburbs of Boston.

```{r}
# to get the data
BHData <- read.table(url("https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"), sep = "")

```

For our study we are working on 506 rows (events) and 14 columns. One of them called `medv` is our target value - `y` or response variable.

```{r}
# knowing the dimension of the data
dim(BHData)
```


## Set names of the dataset

```{r}
# changing the variable's names
names(BHData)<- c("crim","zn","indus","chas","nox","rm",   
       "age","dis","rad","tax","ptratio","black","lstat","medv")
```

## EDA (Exploratory Data Analysis)

Usually as a first task, we use some *Exploratory Data Analysis* to understand how the data is distributed and extract preliminary knowledge.

```{r}
# structure
str(BHData)
```

As you can see using the `summary()` function below, there are variables with different ranges. It is could badly impact the response variable if we have had a less numeric range between each of the predictors variables.

As we have to improve the predictive accuracy of our model we have not allowed that this large difference in the range of variables impact the accuracy of the predicting task upon the `medv` variable.

You will see the adequate treatment in the *Pre-processing* topic.

```{r}
summary(BHData)
```

## Pre-processing

It is an important step called `featured scaling` to get all the data scaled in the range [0,1]. This method has chosen can be called as well as `normalization`. 

```{r}
# calculating the maximun in each column
max_data <- apply(BHData, 2, max)
```

```{r}
# calculating the minimun in each column
min_data <- apply(BHData, 2, min)
```

```{r}
# applying the normalization
BHDataScaled <- as.data.frame(scale(BHData,center = min_data, 
  scale = max_data - min_data))
```

To confirm normalization process:

```{r}
summary(BHDataScaled)
```

```{r}
boxplot(BHDataScaled)
```

According with the graph above there some som variables with outliers. But the `crim` predictor variable has the largest number os outliers.

```{r}
CorBHData<-cor(BHDataScaled)
```

```{r}
library(corrplot)

corrplot(CorBHData, method = "pie",type="lower")
```

## Multiple Linear Model Fitting

```{r}
LModel1<-lm(medv~.,data=BHDataScaled)
```

```{r}
summary(LModel1)
```

```{r}
Pred1 <- predict(LModel1)
```

```{r}
mse1 <- mean((BHDataScaled$medv - Pred1)^2)
```

```{r}
mse1
```

```{r}
plot(BHDataScaled[,14],Pred1,
     xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
```

```{r}
par(mfrow=c(2,2))
plot(LModel1)
```

## Random Forest Regression Model

```{r}
library(randomForest)
```

```{r}
RFModel=randomForest(medv ~ . , data = BHDataScaled)
RFModel
```

```{r}
summary(RFModel)
```

```{r}
plot(RFModel)
```

```{r}
VarImp<-importance(RFModel)
VarImp<-as.matrix(VarImp[order(VarImp[,1], decreasing = TRUE),])
VarImp
```

```{r}
varImpPlot(RFModel)
```

```{r}
Pred2 <- predict(RFModel)
```

```{r}
plot(BHDataScaled[,14],Pred2,
     xlab="Actual",ylab="Predicted")
abline(a=0,b=1)
```

## Robust Linear Regression Model

```{r}
library(MASS)

LModel2 <- rlm(BHDataScaled$medv ~ ., data = BHDataScaled, psi = psi.hampel, init = "lts")
```

```{r}
LModel2
```

```{r}
summary(LModel2)
```

```{r}
LM1Coef <- coef(LModel1)

LM2Coef <- coef(LModel2)
```

```{r}
plot(BHDataScaled$medv, BHDataScaled$lstat)
abline(coef=LM1Coef)
plot(BHDataScaled$medv, BHDataScaled$lstat)
abline(coef=LM2Coef)
```

```{r}
boxplot(BHDataScaled$crim)$out
```

```{r}
outliers <- boxplot(BHDataScaled$crim, plot=FALSE)$out
print(outliers)
```


```{r}
BHDataScaled[which(BHDataScaled$crim %in% outliers),]
```

```{r}
boxplot(BHDataScaled$crim)
```

```{r}
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
```

```{r}
remove_outliers(BHDataScaled$crim)
```

```{r}
BHDataScaled$crim[!BHDataScaled$crim %in% boxplot.stats(BHDataScaled$crim)$out]
```

