---
title: "Supervised Learning with Scikit-Learn"
author: "Salerno"
date: "2020-03-18"
categories: ["Python", "Data Science", "Programming", "Machine Learning"]
tags: []
draft: FALSE
banner: img/banners/python.png
---



<div id="the-iris-dataset-in-scikit-learn" class="section level2">
<h2>1. The Iris dataset in scikit-learn</h2>
<pre class="python"><code>
from sklearn import datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

plt.style.use(&#39;ggplot&#39;)
iris = datasets.load_iris()
type(iris)</code></pre>
<pre><code>## &lt;class &#39;sklearn.utils.Bunch&#39;&gt;</code></pre>
<pre class="python"><code>
print(iris.keys())</code></pre>
<pre><code>## dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;])</code></pre>
<pre class="python"><code>print(iris.DESCR)
</code></pre>
<pre><code>## .. _iris_dataset:
## 
## Iris plants dataset
## --------------------
## 
## **Data Set Characteristics:**
## 
##     :Number of Instances: 150 (50 in each of three classes)
##     :Number of Attributes: 4 numeric, predictive attributes and the class
##     :Attribute Information:
##         - sepal length in cm
##         - sepal width in cm
##         - petal length in cm
##         - petal width in cm
##         - class:
##                 - Iris-Setosa
##                 - Iris-Versicolour
##                 - Iris-Virginica
##                 
##     :Summary Statistics:
## 
##     ============== ==== ==== ======= ===== ====================
##                     Min  Max   Mean    SD   Class Correlation
##     ============== ==== ==== ======= ===== ====================
##     sepal length:   4.3  7.9   5.84   0.83    0.7826
##     sepal width:    2.0  4.4   3.05   0.43   -0.4194
##     petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
##     petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
##     ============== ==== ==== ======= ===== ====================
## 
##     :Missing Attribute Values: None
##     :Class Distribution: 33.3% for each of 3 classes.
##     :Creator: R.A. Fisher
##     :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
##     :Date: July, 1988
## 
## The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
## from Fisher&#39;s paper. Note that it&#39;s the same as in R, but not as in the UCI
## Machine Learning Repository, which has two wrong data points.
## 
## This is perhaps the best known database to be found in the
## pattern recognition literature.  Fisher&#39;s paper is a classic in the field and
## is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
## data set contains 3 classes of 50 instances each, where each class refers to a
## type of iris plant.  One class is linearly separable from the other 2; the
## latter are NOT linearly separable from each other.
## 
## .. topic:: References
## 
##    - Fisher, R.A. &quot;The use of multiple measurements in taxonomic problems&quot;
##      Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to
##      Mathematical Statistics&quot; (John Wiley, NY, 1950).
##    - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
##      (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
##    - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System
##      Structure and Classification Rule for Recognition in Partially Exposed
##      Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine
##      Intelligence, Vol. PAMI-2, No. 1, 67-71.
##    - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions
##      on Information Theory, May 1972, 431-433.
##    - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II
##      conceptual clustering system finds 3 classes in the data.
##    - Many, many more ...</code></pre>
<pre class="python"><code>
type(iris.data), type(iris.target)</code></pre>
<pre><code>## (&lt;class &#39;numpy.ndarray&#39;&gt;, &lt;class &#39;numpy.ndarray&#39;&gt;)</code></pre>
<pre class="python"><code>
iris.data.shape</code></pre>
<pre><code>## (150, 4)</code></pre>
<pre class="python"><code>
iris.target_names</code></pre>
<pre><code>## array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;)</code></pre>
</div>
<div id="exploratory-data-analysis-eda" class="section level2">
<h2>2. Exploratory data analysis (EDA)</h2>
<pre class="python"><code>
X = iris.data
y = iris.target
df = pd.DataFrame(X, columns=iris.feature_names)
print(df.head())</code></pre>
<pre><code>##    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
## 0                5.1               3.5                1.4               0.2
## 1                4.9               3.0                1.4               0.2
## 2                4.7               3.2                1.3               0.2
## 3                4.6               3.1                1.5               0.2
## 4                5.0               3.6                1.4               0.2</code></pre>
</div>
<div id="visual-eda" class="section level2">
<h2>3. Visual EDA</h2>
<pre class="python"><code>
_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8, 8],
s=150, marker = &#39;D&#39;)
plt.savefig(&#39;./_bookdown_files/_main_files/figure-html/iris.png&#39;)
plt.show()</code></pre>
<p><img src="/blog/2020-03-18-supervised-learning-with-scikit-learn_files/figure-html/unnamed-chunk-7-1.png" width="768" /></p>
</div>
<div id="using-scikit-learn-to-fit-a-classifier" class="section level2">
<h2>4. Using scikit-learn to fit a classifier</h2>
<pre class="python"><code>
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=6)

X = iris[&#39;data&#39;]
y = iris[&#39;target&#39;]

# Fit the classifier to the data
knn.fit(X, y)</code></pre>
<pre><code>## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=6, p=2,
##                      weights=&#39;uniform&#39;)</code></pre>
<pre class="python"><code>X.shape</code></pre>
<pre><code>## (150, 4)</code></pre>
<pre class="python"><code>y.shape</code></pre>
<pre><code>## (150,)</code></pre>
</div>
<div id="predicting-on-unlabeled-data" class="section level2">
<h2>5. Predicting on unlabeled data</h2>
<pre class="python"><code>
# Predict the labels for the training data X
y_pred = knn.predict(X)

print(&quot;Prediction: {}&quot;.format(y_pred))</code></pre>
<pre><code>## Prediction: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
##  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1
##  1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2
##  2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  2 2]</code></pre>
</div>
<div id="traintest-split" class="section level2">
<h2>6. Train/Test split</h2>
<pre class="python"><code>
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)
knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(X_train, y_train)</code></pre>
<pre><code>## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=8, p=2,
##                      weights=&#39;uniform&#39;)</code></pre>
<pre class="python"><code>y_pred = knn.predict(X_test)
print(&quot;Test set predictions:\n&quot;, &quot;{}&quot;.format(y_pred))</code></pre>
<pre><code>## Test set predictions:
##  [2 1 2 2 1 0 1 0 0 1 0 2 0 2 2 0 0 0 1 0 2 2 2 0 1 1 1 0 0 1 2 2 0 0 1 2 2
##  1 1 2 1 1 0 2 1]</code></pre>
<pre class="python"><code>
knn.score(X_test, y_test)</code></pre>
<pre><code>## 0.9555555555555556</code></pre>
</div>
<div id="overfitting-and-underfitting" class="section level2">
<h2>7. Overfitting and underfitting</h2>
<pre class="python"><code>
# Setup arrays to store train and test accuracies
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

# Loop over different values of k
for i, k in enumerate(neighbors):
    # Setup a k-NN Classifier with k neighbors: knn
    knn = KNeighborsClassifier(n_neighbors=k)

    # Fit the classifier to the training data
    knn.fit(X_train, y_train)
    
    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(X_train, y_train)

    #Compute accuracy on the testing set
    test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot</code></pre>
<pre><code>## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=1, p=2,
##                      weights=&#39;uniform&#39;)
## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=2, p=2,
##                      weights=&#39;uniform&#39;)
## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=3, p=2,
##                      weights=&#39;uniform&#39;)
## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=4, p=2,
##                      weights=&#39;uniform&#39;)
## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,
##                      weights=&#39;uniform&#39;)
## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=6, p=2,
##                      weights=&#39;uniform&#39;)
## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=7, p=2,
##                      weights=&#39;uniform&#39;)
## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=8, p=2,
##                      weights=&#39;uniform&#39;)</code></pre>
<pre class="python"><code>plt.title(&#39;k-NN: Varying Number of Neighbors&#39;)</code></pre>
<pre><code>## Text(0.5, 1.0, &#39;k-NN: Varying Number of Neighbors&#39;)</code></pre>
<pre class="python"><code>plt.plot(neighbors, test_accuracy, label = &#39;Testing Accuracy&#39;)</code></pre>
<pre><code>## [&lt;matplotlib.lines.Line2D object at 0x0000000012210D48&gt;]</code></pre>
<pre class="python"><code>plt.plot(neighbors, train_accuracy, label = &#39;Training Accuracy&#39;)</code></pre>
<pre><code>## [&lt;matplotlib.lines.Line2D object at 0x000000001225D148&gt;]</code></pre>
<pre class="python"><code>plt.legend()</code></pre>
<pre><code>## &lt;matplotlib.legend.Legend object at 0x0000000012261088&gt;</code></pre>
<pre class="python"><code>plt.xlabel(&#39;Number of Neighbors&#39;)</code></pre>
<pre><code>## Text(0.5, 0, &#39;Number of Neighbors&#39;)</code></pre>
<pre class="python"><code>plt.ylabel(&#39;Accuracy&#39;)</code></pre>
<pre><code>## Text(0, 0.5, &#39;Accuracy&#39;)</code></pre>
<pre class="python"><code>plt.savefig(&#39;./_bookdown_files/_main_files/figure-html/knn.png&#39;)
plt.show()
</code></pre>
<p><img src="/blog/2020-03-18-supervised-learning-with-scikit-learn_files/figure-html/unnamed-chunk-12-1.png" width="768" /></p>
</div>
